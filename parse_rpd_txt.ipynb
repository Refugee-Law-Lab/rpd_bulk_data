{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook to parse text files to produce cleaned text of RPD (Legacy) decisions\n",
    "\n",
    "Sean Rehaag\n",
    "\n",
    "License: Creative Commons Attribution-NonCommercial 4.0 International (CC BY-NC 4.0). \n",
    "\n",
    "Dataset & Code to be cited as:\n",
    "\n",
    "Sean Rehaag, \"Refugee Protection Division (Legacy) Bulk Decisions Dataset\" (2023), online: Refugee Law Laboratory <https://refugeelab.ca/bulk-data/rpd/>.\n",
    "\n",
    "Notes:\n",
    "\n",
    "(1) Data Source: In the fall of 2022 the Immigration and Refugee Board provided the RLL with a full backlog of approximately 116k published decisions from all divisions (RAD, RPD, ID, IAD). Because the IRB no longer regularly publishes RPD decisions, the dataset is no longer being updated, which is why we refer to the \n",
    "dataset as a legacy dataset. For more recent RPD decisions (obtained via Access to Information Requests), \n",
    "see the RLLR dataset.\n",
    "\n",
    "(2) Unofficial Data: The data are unofficial reproductions. For official versions, please contact the Immigration and Refugee Board. \n",
    "\n",
    "(3) Non-Affiliation / Endorsement: The data has been collected and reproduced without any affiliation or endorsement from the Immigration and Refugee Board.\n",
    "\n",
    "(4) Non-Commerical Use: As indicated in the license, data may be used for non-commercial use (with attribution) only. For commercial use, please contact the Immigration and Refugee Board. \n",
    "\n",
    "(5) Accuracy: Data was collected and processed programmatically for the purposes of academic research. While we make best efforts to ensure accuracy, data gathering of this kind inevitably involves errors. As such the data should be viewed as preliminary information aimed to prompt further research and discussion, rather than as definitive information.\n",
    "\n",
    "Acknowledgements: Thanks to Rafael Dolores who coded the initial parsing scripts for the Refugee Appeal Division Bulk Decisions Dataset, which were modified for this datset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langdetect\n",
    "#!pip install regex\n",
    "#!pip install dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import regex as re \n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from langdetect import detect, DetectorFactory\n",
    "from difflib import get_close_matches\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import chardet\n",
    "import dask.bag as db\n",
    "from dask.diagnostics import ProgressBar\n",
    "import pathlib\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declaring Constant\n",
    "Here, we specify the directories containing our data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIRS = [\"../IRB Decisions - Initial Request - TEXT\"]\n",
    "\n",
    "# For SR:\n",
    "DATA_DIRS = [\"d:/IRB Decisions - Initial Request - TEXT/\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed for langdetect for consistent results and reproducibility\n",
    "DetectorFactory.seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Language Detection\n",
    "This function determines the language of a given text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def detect_language(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Maker Extraction\n",
    "This function searches the given file for the decision maker using regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_decision_maker(content):\n",
    "    patterns = [\n",
    "        # String in line immediately after 'Panel' and before 'Tribunal', allowing tabs and spaces\n",
    "        r\"^Panel\\s*([^\\n]+?)\\s*\\n\\s*Tribunal\\s*$\",  \n",
    "      \n",
    "        # String in line immediately after 'Tribunal' and before 'Panel', allowing tabs and spaces\n",
    "        r\"^Tribunal\\s*([^\\n]+?)\\s*\\n\\s*Panel\\s*$\",\n",
    "        # String in line immediately after 'Tribunal' and followed by another 'Tribunal', allowing tabs and spaces\n",
    "        r\"^Tribunal\\s*([^\\n]+?)\\s*\\n\\s*Tribunal\\s*$\"\n",
    "    ]\n",
    "\n",
    "    for pattern in patterns:\n",
    "        # Use re.MULTILINE to allow ^ and $ to match the start and end of each line\n",
    "        match = re.search(pattern, content, re.IGNORECASE | re.MULTILINE)\n",
    "        if match:\n",
    "            captured = match.group(1).strip()\n",
    "            # Check if captured group ends with 'Tribunal' or 'Panel' and exclude it\n",
    "            if not captured.endswith(\"Tribunal\") and not captured.endswith(\"Panel\"):\n",
    "                return captured\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Regular Expression Detector\n",
    "Functions to parse the date from text files while accounting for several different formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_date_patterns(content):\n",
    "    patterns = {\n",
    "        \"custom\": (r\"Date (?:of decision|de la décision)\\s*\\n\\s*([A-Za-z]+)\\s+(\\d{1,2})\\.\\s*(\\d{4})\", lambda m: [m.group(1), m.group(2), m.group(3)]),\n",
    "        \"primary\": (r\"Date (?:of decision|de la décision)\\s*(?:Le )?\\s*((?:(?:\\d{1,2}|1er)\\s+[\\w]+\\s*,?\\s*\\d{1,4})|\\w+\\s+\\d{1,2}(?:st|nd|rd|th)?\\s*,?\\s*\\d{1,4}|\\d{1,2}-\\d{1,2}-\\d{1,4})\", lambda m: m.group(1).replace(',', '').split()),\n",
    "        \"original_decision\": (r\"Date of decision\\s+([\\w\\s]+),\\s+(\\d{4})\\s+\\(original decision\\)\", lambda m: m.group(1).strip().split() + [m.group(2).strip()]),\n",
    "        \"tribunal\": (r\"Tribunal\\s*\\n\\s*([\\w\\s]+?)\\s*\\n\\s*Date of decision\", lambda m: m.group(1).replace(',', '').split()),\n",
    "        \"original\": (r\"Original\\s+([\\w]+\\s+\\d{1,2}(?:st|nd|rd|th)?,?\\s+\\d{4})\", lambda m: m.group(1).replace(',', '').split())\n",
    "    }\n",
    "\n",
    "    for key, (pattern, process) in patterns.items():\n",
    "        match = re.search(pattern, content, re.IGNORECASE)\n",
    "        if match:\n",
    "            return process(match)\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date Formatter\n",
    "Takes detected regular expression and converts into one common format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "french_to_english = {\n",
    "        'janvier': 'January', 'fevrier': 'February', 'mars': 'March', 'avril': 'April',\n",
    "        'mai': 'May', 'juin': 'June', 'juillet': 'July', 'aout': 'August',\n",
    "        'septembre': 'September', 'octobre': 'October', 'novembre': 'November', 'decembre': 'December'\n",
    "}\n",
    "\n",
    "def correct_month_name(misspelled_month, possibilities=['Janvier','January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December'], cutoff=0.6):\n",
    "    correct_months = get_close_matches(misspelled_month, possibilities, n=1, cutoff=cutoff)\n",
    "    if correct_months:\n",
    "        corrected_month = correct_months[0]\n",
    "        # Check if the corrected month is in the French to English mapping\n",
    "        return french_to_english.get(corrected_month.lower(), corrected_month)\n",
    "    else:\n",
    "        return misspelled_month\n",
    "\n",
    "def correct_year_typo(year):\n",
    "    if len(year) == 3 and year.startswith(\"0\"):\n",
    "        return \"20\" + year[1:]\n",
    "    return year\n",
    "\n",
    "def correct_year_typo(year):\n",
    "    \"\"\"Corrects year format typos.\"\"\"\n",
    "    return \"20\" + year[1:] if len(year) == 3 and year.startswith(\"0\") else year\n",
    "\n",
    "def process_numeric_format(parts):\n",
    "    \"\"\"Processes numeric date format 'dd-mm-yyyy'.\"\"\"\n",
    "    day, month, year = parts[0].split('-')\n",
    "    year = correct_year_typo(year)\n",
    "    return datetime(int(year), int(month), int(day)).date().strftime('%Y-%m-%d')\n",
    "\n",
    "def process_day_first_format(parts, french_month_mapping):\n",
    "    \"\"\"Processes dates in 'day month year' format, French or English.\"\"\"\n",
    "    day = 1 if parts[0].lower() == '1er' else int(parts[0])\n",
    "\n",
    "    month = ''\n",
    "    # Check if month and year are concatenated\n",
    "    if len(parts) == 2 and not parts[1].isdigit():\n",
    "        month_year_str = parts[1]\n",
    "        for i in range(1, len(month_year_str)):\n",
    "            if month_year_str[i:].isdigit():\n",
    "                month_str, year_str = month_year_str[:i], month_year_str[i:]\n",
    "                year = correct_year_typo(year_str)\n",
    "                month = french_month_mapping.get(month_str.lower().replace('é', 'e').replace('û', 'u').replace('ô', 'o'), month_str.capitalize())\n",
    "                break\n",
    "    else:\n",
    "        month = parts[1].lower().replace('é', 'e').replace('û', 'u').replace('ô', 'o')\n",
    "        if len(parts) >= 3:\n",
    "            year = correct_year_typo(parts[2])\n",
    "\n",
    "    if month in french_month_mapping:\n",
    "        return datetime(int(year), french_month_mapping[month], day).date().strftime('%Y-%m-%d')\n",
    "    else:\n",
    "        if isinstance(month, int):\n",
    "            return datetime(int(year), month, day).date().strftime('%Y-%m-%d')\n",
    "        \n",
    "        corrected_month = correct_month_name(month.capitalize())\n",
    "        try:\n",
    "            return datetime.strptime(f\"{corrected_month} {day} {year}\", '%B %d %Y').date().strftime('%Y-%m-%d')\n",
    "        except ValueError as e:\n",
    "            print(f\"Error parsing date: {e}\")\n",
    "            return None\n",
    "\n",
    "def process_month_first_format(parts):\n",
    "    \"\"\"Processes month first format with possible ordinal suffix.\"\"\"\n",
    "    day = 0\n",
    "    month = ''\n",
    "    year = ''\n",
    "    \n",
    "    if len(parts) == 2 and parts[1].isdigit() and len(parts[1]) > 2:\n",
    "        \n",
    "        if parts[1].isdigit() and len(parts[1]) > 4: \n",
    "            month = parts[0]\n",
    "            year_str = parts[1][-4:]\n",
    "            day_str = parts[1][:-4]\n",
    "            year = year_str\n",
    "            day = int(day_str)\n",
    "            \n",
    "        elif parts[1].isdigit()and len(parts[1]) > 3: #Year is the second entry\n",
    "            month_day_str = parts[0]\n",
    "            for i in range(1, len(month_day_str)):\n",
    "                if not month_day_str[i].isdigit():\n",
    "                    day_str, month_str = month_day_str[:i], month_day_str[i:]\n",
    "                    if day_str.isdigit():\n",
    "                        day = int(day_str)\n",
    "                    month = french_to_english.get(month_str.lower().replace('é', 'e').replace('û', 'u').replace('ô', 'o'), month_str)\n",
    "                    parts[0] = month\n",
    "                    break\n",
    "            year = parts[1]\n",
    "        else:\n",
    "            year_str = parts[1][-4:]\n",
    "            day_str = parts[1][:-4]\n",
    "            year = correct_year_typo(year_str)\n",
    "            day = int(day_str)\n",
    "\n",
    "    else:\n",
    "        day = re.sub(r\"[^\\d]\", \"\", parts[1])\n",
    "        day = int(day) if day.isdigit() else 1\n",
    "        if len(parts) >= 3:\n",
    "            year = correct_year_typo(parts[2])\n",
    "            \n",
    "    try:\n",
    "        corrected_month = correct_month_name(parts[0].capitalize())\n",
    "        return datetime.strptime(f\"{corrected_month} {day} {year}\", '%B %d %Y').date().strftime('%Y-%m-%d')\n",
    "    except ValueError as e:\n",
    "        print(f\"Error parsing date: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Date Extraction\n",
    "This function searches the given file for the document date using regular expressions, taking into account both French and English texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_date_parts(parts, french_month_mapping):\n",
    "    \"\"\"Determines the correct date processing method based on the format of the parts.\"\"\"\n",
    "    if '-' in parts[0]:\n",
    "        return process_numeric_format(parts)\n",
    "    elif parts[0].isdigit() or parts[0].lower() == '1er':\n",
    "        return process_day_first_format(parts, french_month_mapping)\n",
    "    else:\n",
    "        return process_month_first_format(parts)\n",
    "\n",
    "def extract_document_date(content):\n",
    "    french_month_mapping = {\n",
    "        'janvier': 1, 'fevrier': 2, 'mars': 3, 'avril': 4,\n",
    "        'mai': 5, 'juin': 6, 'juillet': 7, 'aout': 8,\n",
    "        'septembre': 9, 'octobre': 10, 'novembre': 11, 'decembre': 12\n",
    "    }\n",
    "    \n",
    "    parts = match_date_patterns(content)\n",
    "    \n",
    "    if not parts:\n",
    "        return None\n",
    "    return process_date_parts(parts, french_month_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Processor Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_rpd_number(content):\n",
    "    \"\"\"Extracts the RPD number from the content, ignoring IAD/ID/RAD files.\"\"\"\n",
    "    # Check for lines indicating the file should be ignored\n",
    "    ignore_lines = [\"IAD File\",\n",
    "                    \"dossier de la SAI\",\n",
    "                    \"IMMIGRATION APPEAL DIVISION\", \n",
    "                    \"ID File\", \n",
    "                    \"IMMIGRATION DIVISION\", \n",
    "                    \"RAD File\",\n",
    "                    \"REFUGEE APPEAL DIVISION\",\n",
    "                    \"REFUGEE DIVISION\"\n",
    "                    ]\n",
    "    \n",
    "    for line in content.splitlines():\n",
    "        # Sanitize the current line\n",
    "        sanitized_line = ''.join(c for c in line if c.isprintable()).strip()\n",
    "\n",
    "        # Check if the sanitized line matches any ignore line\n",
    "        if any(ignore_line in sanitized_line for ignore_line in ignore_lines):   \n",
    "            return None\n",
    "        \n",
    "        if \"RPD File\" in sanitized_line or \"RPD file\" in sanitized_line :          \n",
    "            rpd_number_match = re.search(r\"([A-Z]{2}\\d+.\\d+)\", sanitized_line)\n",
    "            if rpd_number_match:\n",
    "                return rpd_number_match.group(1)\n",
    "            \n",
    "            # If RPD is in the next immediate line\n",
    "            next_line_index = content.splitlines().index(line) + 1\n",
    "            if next_line_index < len(content.splitlines()):\n",
    "                next_line = content.splitlines()[next_line_index]\n",
    "                rpd_number_match = re.search(r\"([A-Z]{2}\\d+-\\d+)\", next_line)\n",
    "                if rpd_number_match:\n",
    "                    return rpd_number_match.group(1)\n",
    "\n",
    "    if \"REFUGEE PROTECTION DIVISION\" in content or \"SECTION DE LA PROTECTION DES RÉFUGIÉS\" in content:\n",
    "        rpd_number_match = re.search(r\"([A-Z]{2}\\d+-\\d+)\", content)\n",
    "        if rpd_number_match:\n",
    "            return rpd_number_match.group(1)\n",
    "\n",
    "    return None\n",
    "\n",
    "def process_file(file_path):\n",
    "    \"\"\"Processes a single file and extracts data.\"\"\"\n",
    "\n",
    "    # manually exclude problematic files\n",
    "    problem_files = [\"1821419\", \"636351\"]\n",
    "    if Path(file_path).stem in problem_files:\n",
    "        return None\n",
    "    \n",
    "    # Use chardet to detect the encoding of the file\n",
    "    with open(file_path, 'rb') as file:\n",
    "        raw_data = file.read()\n",
    "    encoding = chardet.detect(raw_data)['encoding']\n",
    "\n",
    "    # Read the file with the detected encoding\n",
    "    with open(file_path, 'r', errors='replace', encoding=encoding) as file:\n",
    "        content = file.read()\n",
    "\n",
    "    rpd_number = extract_rpd_number(content)\n",
    "    if rpd_number:\n",
    "        lang = detect_language(content)\n",
    "        decision_maker_name = extract_decision_maker(content)\n",
    "        document_date = extract_document_date(content)\n",
    "        year = int(document_date.split('-')[0]) if document_date else None\n",
    "\n",
    "        return {\n",
    "            'citation': rpd_number,\n",
    "            'citation2': '',\n",
    "            'dataset': 'RPD',\n",
    "            'name': '',\n",
    "            'source_url': os.path.basename(file_path),\n",
    "            #'scraped_timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'scraped_timestamp': datetime.fromtimestamp(os.path.getmtime(file_path)).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'document_date': document_date,\n",
    "            'year': year,\n",
    "            'unofficial_text': content,\n",
    "            'language': lang,\n",
    "            'other': json.dumps({'decision-maker_name': decision_maker_name}, ensure_ascii=False),\n",
    "        }\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Files\n",
    "This block of code reads each file in the dataset directories to extract the needed information, using the previously defined functions and form a Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 201.94 s\n"
     ]
    }
   ],
   "source": [
    "# Main data processing loop (run in paraellel using Dask)\n",
    "\n",
    "def process_file_wrapper(file_path):\n",
    "    if not os.path.basename(file_path).startswith('~'):\n",
    "        return process_file(file_path)\n",
    "\n",
    "# Gather all file paths\n",
    "file_paths = []\n",
    "for data_dir in DATA_DIRS:\n",
    "    if os.path.exists(data_dir) and os.path.isdir(data_dir):\n",
    "        dir_files = [os.path.join(data_dir, f) for f in os.listdir(data_dir)]\n",
    "        file_paths.extend(dir_files)\n",
    "\n",
    "# Create a Dask Bag from file paths\n",
    "file_bag = db.from_sequence(file_paths)\n",
    "\n",
    "# Use Dask to process files in parallel\n",
    "with ProgressBar():\n",
    "    results = file_bag.map(process_file_wrapper).filter(lambda x: x is not None).compute()\n",
    "\n",
    "# Convert results to a Pandas DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# # Process files using a regular loop for easier debugging if needed\n",
    "# results = []\n",
    "# for file_path in file_paths:\n",
    "#     try:\n",
    "#         result = process_file_wrapper(file_path)\n",
    "#         if result is not None:\n",
    "#             results.append(result)\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error processing file {file_path}: {e}\")\n",
    "\n",
    "# # Convert results to a Pandas DataFrame\n",
    "# df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning\n",
    "Cleans data to match huggingface dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13833\n",
      "12472\n",
      "12470\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>citation</th>\n",
       "      <th>citation2</th>\n",
       "      <th>dataset</th>\n",
       "      <th>name</th>\n",
       "      <th>source_url</th>\n",
       "      <th>scraped_timestamp</th>\n",
       "      <th>document_date</th>\n",
       "      <th>year</th>\n",
       "      <th>unofficial_text</th>\n",
       "      <th>language</th>\n",
       "      <th>other</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MA6-05104</td>\n",
       "      <td></td>\n",
       "      <td>RPD</td>\n",
       "      <td></td>\n",
       "      <td>1005371.txt</td>\n",
       "      <td>2023-11-12</td>\n",
       "      <td>2008-04-04</td>\n",
       "      <td>2008</td>\n",
       "      <td>Commission de l'immigration et du statut de ré...</td>\n",
       "      <td>fr</td>\n",
       "      <td>{\"decision-maker_name\": null}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TA6-12745</td>\n",
       "      <td></td>\n",
       "      <td>RPD</td>\n",
       "      <td></td>\n",
       "      <td>1005377.txt</td>\n",
       "      <td>2023-11-12</td>\n",
       "      <td>2008-07-03</td>\n",
       "      <td>2008</td>\n",
       "      <td>\\n\\n\\nRPD File No. / N° de dossier de la SPR :...</td>\n",
       "      <td>en</td>\n",
       "      <td>{\"decision-maker_name\": \"Joanna Bedard\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TA8-21155</td>\n",
       "      <td></td>\n",
       "      <td>RPD</td>\n",
       "      <td></td>\n",
       "      <td>1005397.txt</td>\n",
       "      <td>2023-11-12</td>\n",
       "      <td>2009-10-27</td>\n",
       "      <td>2009</td>\n",
       "      <td>\\n\\n\\n\\nN° de dossier de la SPR/RPD File No.: ...</td>\n",
       "      <td>fr</td>\n",
       "      <td>{\"decision-maker_name\": \"Cliff Berry\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MA6-00156</td>\n",
       "      <td></td>\n",
       "      <td>RPD</td>\n",
       "      <td></td>\n",
       "      <td>1005435.txt</td>\n",
       "      <td>2023-11-12</td>\n",
       "      <td>2008-06-06</td>\n",
       "      <td>2008</td>\n",
       "      <td>Commission de l'immigration\\net du statut de r...</td>\n",
       "      <td>fr</td>\n",
       "      <td>{\"decision-maker_name\": null}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TA7-02522</td>\n",
       "      <td></td>\n",
       "      <td>RPD</td>\n",
       "      <td></td>\n",
       "      <td>1005448.txt</td>\n",
       "      <td>2023-11-12</td>\n",
       "      <td>2009-09-17</td>\n",
       "      <td>2009</td>\n",
       "      <td>\\n\\nRPD File No. / N° de dossier de la SPR :  ...</td>\n",
       "      <td>en</td>\n",
       "      <td>{\"decision-maker_name\": \"M. McCaffrey\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12462</th>\n",
       "      <td>TA6-11195</td>\n",
       "      <td></td>\n",
       "      <td>RPD</td>\n",
       "      <td></td>\n",
       "      <td>993958.txt</td>\n",
       "      <td>2023-11-13</td>\n",
       "      <td>2008-06-04</td>\n",
       "      <td>2008</td>\n",
       "      <td>\\n\\n\\nN° de dossier de la SPR/RPD File No.: TA...</td>\n",
       "      <td>fr</td>\n",
       "      <td>{\"decision-maker_name\": \"E. Joanne Sajtos\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12463</th>\n",
       "      <td>TA7-14136</td>\n",
       "      <td></td>\n",
       "      <td>RPD</td>\n",
       "      <td></td>\n",
       "      <td>993959.txt</td>\n",
       "      <td>2023-11-13</td>\n",
       "      <td>2008-12-02</td>\n",
       "      <td>2008</td>\n",
       "      <td>\\n\\n\\nRPD File No. / N° de dossier de la SPR :...</td>\n",
       "      <td>en</td>\n",
       "      <td>{\"decision-maker_name\": \"Roslyn Ahara\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12464</th>\n",
       "      <td>TA7-10081</td>\n",
       "      <td></td>\n",
       "      <td>RPD</td>\n",
       "      <td></td>\n",
       "      <td>993961.txt</td>\n",
       "      <td>2023-11-13</td>\n",
       "      <td>2008-08-15</td>\n",
       "      <td>2008</td>\n",
       "      <td>\\n\\n\\nN° de dossier de la SPR/RPD File No.: TA...</td>\n",
       "      <td>fr</td>\n",
       "      <td>{\"decision-maker_name\": \"Ken Atkinson\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12465</th>\n",
       "      <td>MA7-01235</td>\n",
       "      <td></td>\n",
       "      <td>RPD</td>\n",
       "      <td></td>\n",
       "      <td>997313.txt</td>\n",
       "      <td>2023-11-13</td>\n",
       "      <td>2010-12-13</td>\n",
       "      <td>2010</td>\n",
       "      <td>Immigration and\\nRefugee Board\\nRefugee Protec...</td>\n",
       "      <td>en</td>\n",
       "      <td>{\"decision-maker_name\": null}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12466</th>\n",
       "      <td>MA7-01235</td>\n",
       "      <td></td>\n",
       "      <td>RPD</td>\n",
       "      <td></td>\n",
       "      <td>997318.txt</td>\n",
       "      <td>2023-11-13</td>\n",
       "      <td>2010-12-13</td>\n",
       "      <td>2010</td>\n",
       "      <td>Immigration and\\nRefugee Board\\nRefugee Protec...</td>\n",
       "      <td>fr</td>\n",
       "      <td>{\"decision-maker_name\": null}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12467 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        citation citation2 dataset name   source_url scraped_timestamp   \n",
       "0      MA6-05104               RPD       1005371.txt        2023-11-12  \\\n",
       "1      TA6-12745               RPD       1005377.txt        2023-11-12   \n",
       "2      TA8-21155               RPD       1005397.txt        2023-11-12   \n",
       "3      MA6-00156               RPD       1005435.txt        2023-11-12   \n",
       "4      TA7-02522               RPD       1005448.txt        2023-11-12   \n",
       "...          ...       ...     ...  ...          ...               ...   \n",
       "12462  TA6-11195               RPD        993958.txt        2023-11-13   \n",
       "12463  TA7-14136               RPD        993959.txt        2023-11-13   \n",
       "12464  TA7-10081               RPD        993961.txt        2023-11-13   \n",
       "12465  MA7-01235               RPD        997313.txt        2023-11-13   \n",
       "12466  MA7-01235               RPD        997318.txt        2023-11-13   \n",
       "\n",
       "      document_date  year                                    unofficial_text   \n",
       "0        2008-04-04  2008  Commission de l'immigration et du statut de ré...  \\\n",
       "1        2008-07-03  2008  \\n\\n\\nRPD File No. / N° de dossier de la SPR :...   \n",
       "2        2009-10-27  2009  \\n\\n\\n\\nN° de dossier de la SPR/RPD File No.: ...   \n",
       "3        2008-06-06  2008  Commission de l'immigration\\net du statut de r...   \n",
       "4        2009-09-17  2009  \\n\\nRPD File No. / N° de dossier de la SPR :  ...   \n",
       "...             ...   ...                                                ...   \n",
       "12462    2008-06-04  2008  \\n\\n\\nN° de dossier de la SPR/RPD File No.: TA...   \n",
       "12463    2008-12-02  2008  \\n\\n\\nRPD File No. / N° de dossier de la SPR :...   \n",
       "12464    2008-08-15  2008  \\n\\n\\nN° de dossier de la SPR/RPD File No.: TA...   \n",
       "12465    2010-12-13  2010  Immigration and\\nRefugee Board\\nRefugee Protec...   \n",
       "12466    2010-12-13  2010  Immigration and\\nRefugee Board\\nRefugee Protec...   \n",
       "\n",
       "      language                                        other  \n",
       "0           fr                {\"decision-maker_name\": null}  \n",
       "1           en     {\"decision-maker_name\": \"Joanna Bedard\"}  \n",
       "2           fr       {\"decision-maker_name\": \"Cliff Berry\"}  \n",
       "3           fr                {\"decision-maker_name\": null}  \n",
       "4           en      {\"decision-maker_name\": \"M. McCaffrey\"}  \n",
       "...        ...                                          ...  \n",
       "12462       fr  {\"decision-maker_name\": \"E. Joanne Sajtos\"}  \n",
       "12463       en      {\"decision-maker_name\": \"Roslyn Ahara\"}  \n",
       "12464       fr      {\"decision-maker_name\": \"Ken Atkinson\"}  \n",
       "12465       en                {\"decision-maker_name\": null}  \n",
       "12466       fr                {\"decision-maker_name\": null}  \n",
       "\n",
       "[12467 rows x 11 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Manually fix some dates:\n",
    "\n",
    "# if df.doccument_date are in a series of problem dates, then drop the row from the df\n",
    "problem_dates = [\"0013-10-11\",\n",
    "                 \"0013-05-31\", \n",
    "                 \"3006-03-03\", \n",
    "                 \"1005-10-04\", \n",
    "                 \"3013-08-13\",\n",
    "                 \"1015-06-03\",\n",
    "                 \"1006-03-31\"\n",
    "                 ]\n",
    "df = df[~df['document_date'].isin(problem_dates)]\n",
    "\n",
    "# fix dates format\n",
    "df['document_date'] = pd.to_datetime(df['document_date']).dt.strftime('%Y-%m-%d')\n",
    "df['scraped_timestamp'] = pd.to_datetime(df['scraped_timestamp']).dt.strftime('%Y-%m-%d')\n",
    "\n",
    "print (len(df))\n",
    "# drop where year is nan\n",
    "df = df.dropna(subset=['year'])\n",
    "print (len(df))\n",
    "# convert year to int\n",
    "df['year'] = df['year'].astype(int)\n",
    "\n",
    "# Remove rows where unofficial text is duplicated, keeping the last one\n",
    "df = df.drop_duplicates(subset=['unofficial_text'], keep='last')\n",
    "print (len(df))\n",
    "\n",
    "# Remove rows where year is before 2001 or after 2020\n",
    "df = df[(df['year'] > 2001) & (df['year'] <= 2020)]\n",
    "\n",
    "# reset index\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12467/12467 [00:16<00:00, 762.76it/s]\n"
     ]
    }
   ],
   "source": [
    "# Clean text of cases\n",
    "def clean_text(text):\n",
    "\n",
    "    # remove \\xa0\n",
    "    text = text.replace('\\xa0', ' ')\n",
    "\n",
    "    # Remove multiple whitespaces and preserve paragraphs\n",
    "    text = '\\n'.join([re.sub(r'\\s+', ' ', line.strip()) for line in text.split('\\n')])\n",
    "    \n",
    "    # # Remove single newlines\n",
    "    # text = re.sub(r'(?<!\\n)\\n(?!\\n)', ' ', text)\n",
    "\n",
    "    # Convert multiple newlines to single newlines\n",
    "    text = re.sub(r'\\n+', '\\n', text)\n",
    "\n",
    "    # Remove all strings '\\n[Page #]\\n' (with # being a number of up to 4 digits \n",
    "    text = re.sub(r'\\n\\[Page \\d{1,3}\\]\\n', ' ', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "tqdm.pandas()\n",
    "df['unofficial_text'] = df.unofficial_text.progress_apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export cleaned df to jsonl\n",
    "out_path_parsed = pathlib.Path('DATA/rpd_cases.jsonl')\n",
    "df.to_json(out_path_parsed, orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 31.26it/s]\n"
     ]
    }
   ],
   "source": [
    "# get start and end year\n",
    "start_year = df['year'].min()\n",
    "end_year = df['year'].max()\n",
    "\n",
    "# set output dir\n",
    "out_path_yearly = 'DATA/YEARLY/'\n",
    "\n",
    "# export cleaned df to yearly / language json files\n",
    "for year in tqdm(range(start_year, end_year+1)):\n",
    "    for language in ['en', 'fr']:\n",
    "        out_path_yearly_lang = out_path_yearly + f'{year}_{language}.json'\n",
    "        df[(df.year == year) & (df.language == language)].to_json(out_path_yearly_lang, orient='records', indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace 'other' column with empty string for huggingface\n",
    "df['other'] = ''\n",
    "\n",
    "# eport to parquet for hugginface\n",
    "out_path_parquet = pathlib.Path('DATA/rpd_cases.parquet')\n",
    "df.to_parquet(out_path_parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "year\n",
       "2012    1639\n",
       "2011    1557\n",
       "2010    1265\n",
       "2013    1262\n",
       "2009     938\n",
       "2014     897\n",
       "2007     872\n",
       "2006     850\n",
       "2008     744\n",
       "2005     551\n",
       "2015     455\n",
       "2018     340\n",
       "2017     255\n",
       "2003     238\n",
       "2016     194\n",
       "2004     135\n",
       "2019     133\n",
       "2002     121\n",
       "2020      21\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list rows per df.year\n",
    "df['year'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
